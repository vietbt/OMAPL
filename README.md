# O-MAPL: Offline Multi-agent Preference Learning

This repository provides the official implementation for the paper "O-MAPL: Offline Multi-agent Preference Learning," accepted at the International Conference on Machine Learning (ICML) 2025. O-MAPL is a novel end-to-end preference-based learning framework for cooperative multi-agent reinforcement learning (MARL). It innovatively leverages the underlying connection between reward functions and soft Q-functions, eliminating the need for explicit reward modeling.  This approach directly learns policies from preference data—either rule-based or generated by Large Language Models (LLMs)—and employs a carefully designed multi-agent value decomposition strategy to enhance training efficiency and stability. 

O-MAPL has been extensively evaluated on challenging benchmarks including SMACv1, SMACv2, and MaMujoco, demonstrating superior performance over existing methods across a variety of tasks. 

## Key Contributions

* **End-to-End Preference-Based MARL:** Introduces O-MAPL, a single-phase learning approach that directly trains agents' policies from preference data without an explicit reward model. 
* **Novel Value Factorization:** Proposes a value factorization method that ensures scalability in multi-agent environments while preserving the alignment between global and local objectives for stable and effective learning.  This method guarantees convexity in the learning objectives within the Q-space under linear mixing structures, contributing to stable optimization. 
* **Weighted Behavior Cloning for Policy Extraction:** Implements a weighted behavior cloning (WBC) approach for local policy extraction that preserves global-local consistency (GLC) and ensures the validity of extracted local policies, even with nonlinear mixing structures. 
* **LLM Integration for Preference Annotation:** Demonstrates the effective use of LLMs (e.g., GPT-4o) for generating rich and cost-effective preference datasets, significantly improving environment understanding and policy learning in complex multi-agent tasks. 
* **State-of-the-Art Performance:** Achieves superior results on SMACv1, SMACv2, and MaMujoco benchmarks using both rule-based and LLM-generated preference data. 

## Project Structure

```
.
├── algos                 # Implementation of O-MAPL and baseline algorithms
│   ├── bc.py             # Behavioral Cloning (BC)
│   ├── iipl.py           # Independent Inverse Preference Learning (IIPL)
│   ├── iplvdn.py         # IPL with VDN aggregation
│   ├── omarl.py          # O-MAPL implementation
│   ├── slmarl.py         # Supervised Learning for MARL (2-phase approach)
│   └── utils.py          # Utilities for algorithms
├── envs                  # Environment wrappers for SMACv1, SMACv2, and MaMujoco
│   ├── mamujoco          # Multi-agent MuJoCo environments
│   ├── smacv1            # SMACv1 environments
│   └── smacv2            # SMACv2 environments
├── trainers              # Training scripts for each algorithm
├── rollouts              # Rollout utilities for continuous and discrete environments
├── saved_results_final   # Saved experimental results (rule-based and LLM-based)
├── graphs                # Pre-generated graphs for analysis
├── analyze.ipynb         # Jupyter notebook for analysis and visualization
├── llm_generate.py       # Script to generate prompts for LLM-based annotations
├── llm_upload.py         # Script to upload prompts to OpenAI APIs
├── llm_retrieve.py       # Script to retrieve LLM-based annotation results
├── main.py               # Main training script
├── evaluate.py           # Evaluation script
├── config.py             # Configuration file
├── utils.py              # General utilities
└── readme.md             # This readme file
```

## Installation

1.  **Clone the repository:**
    ```bash
    git clone [https://github.com/vietbt/OMAPL.git](https://github.com/vietbt/OMAPL.git)
    cd omapl
    ```

2.  **Create and activate a conda environment (recommended):**
    ```bash
    conda env create -f environment.yml
    conda activate omapl_env
    ```
    Alternatively, install dependencies using pip (ensure Python 3.8+):
    ```bash
    pip install -r requirements.txt
    ```

3.  **Set up OpenAI API Access (for LLM-based dataset generation):**
    Export your OpenAI API key as an environment variable:
    ```bash
    export OPENAI_API_KEY="your-api-key"
    ```

## Usage

### 1. Dataset Generation (LLM-based Preferences)

To generate preference datasets using LLM annotations (e.g., with GPT-4o ):

* **Generate Prompts:** Create prompts from trajectory data. These prompts are constructed using global state information from each trajectory. 
    ```bash
    python -u llm_generate.py
    ```
* **Upload Prompts to OpenAI:** Submit the generated prompts to the OpenAI API. The script handles batching and token management. 
    ```bash
    python -u llm_upload.py
    ```
* **Retrieve Annotations:** Fetch the LLM's preference judgments.
    ```bash
    python -u llm_retrieve.py
    ```
The resulting preference data will be stored for use in training.

### 2. Training

Train O-MAPL or baseline algorithms on the supported environments.

* **MaMujoco Example (e.g., Ant-v2):**
    ```bash
    python -u main.py --algo OMAPL --env_name Ant-v2 --seed 0 --lr 1e-4 --action_scale 0.7
    ```

* **SMACv1 Example (e.g., 5m\_vs\_6m):**
    ```bash
    python -u main.py --algo OMAPL --env_name 5m_vs_6m --seed 0 --batch_size 8 --lr 1e-4
    ```

* **SMACv2 Example (e.g., protoss\_5\_vs\_5):**
    ```bash
    python -u main.py --algo OMAPL --env_name protoss_5_vs_5 --seed 0 --batch_size 8 --lr 1e-4
    ```

* **Training with LLM-based Datasets:**
    Add the `--use-llm` flag to utilize LLM-generated preference data:
    ```bash
    python -u main.py --algo OMAPL --env_name protoss_5_vs_5 --use-llm --seed 0 --batch_size 8 --lr 1e-4
    ```
    Refer to `config.py` and the paper's appendix for hyperparameter details. 

### 3. Evaluation

Evaluate trained models using the `evaluate.py` script. Specify the algorithm, environment, seed, and evaluation step.

```bash
python -u evaluate.py --algo {algo_name} --env_name {environment_name} --seed {seed_number} --eval_step {training_step_to_evaluate}
```

### 4. Analysis and Visualization

The `analyze.ipynb` Jupyter notebook provides tools for in-depth analysis:
* Load and process saved experimental results (from `saved_results_final/`).
* Generate plots for performance metrics (e.g., mean returns, win rates).
* Create tables for quantitative comparisons.

Pre-generated graphs are available in the `graphs/` directory for quick reference.

## Datasets

O-MAPL utilizes offline datasets of pairwise trajectory preferences. 

### Rule-Based Preference Data
* Generated by sampling trajectory pairs from offline datasets of varying quality (e.g., poor, medium, expert). 
* Binary preference labels are assigned based on the source dataset quality, following methodologies similar to IPL. 
* Covers SMACv1, SMACv2, and MaMujoco environments. 

### LLM-Based Preference Data
* Generated by prompting LLMs (specifically GPT-4o in the paper ) with detailed information extracted from trajectory states. This includes metrics like remaining health, shields, relative positions, cooldown times, agent types, and action meanings. 
* This method provides richer, more nuanced preference signals.
* Successfully applied to SMACv1 and SMACv2 tasks.  (Note: MaMujoco states lacked the detailed information required for this LLM-prompting approach. )
* **Example Prompt Snippet (for SMAC):**
    ```
    Scenario: 5m_vs_6m
    Allied Team Agent Configuration: five Marines
    Enemy Team Agent Configuration: six Marines
    Objective: Defeat all enemy agents while ensuring as many allied agents as possible survive.
    Important Notice: You should prefer the trajectory where our allies' health is preserved while significantly reducing the enemy's health. In similar situations, you should prefer shorter trajectory lengths.

    [Trajectory 1]
    1. Final State Information
       1) Allied Agents Health: 0.000, 0.000, 0.067, 0.067, 0.000
       2) Enemy Agents Health: 0.000, 0.000, 0.000, 0.000, 0.000, 0.040
       ...
    2. Total Number of Steps: 28

    [Trajectory 2]
    ...

    Your task is to inform which one is better between [Trajectory1] and [Trajectory2].
    ```
    (For full prompt details, refer to Table 5 in the paper's appendix)

## Implemented Algorithms

* **O-MAPL (Proposed Method):** End-to-end offline multi-agent preference learning. 
* **Baselines:**
    * **BC (Behavioral Cloning):** Imitates preferred trajectories directly. 
    * **IIPL (Independent IPL):** Applies single-agent IPL independently to each agent. 
    * **IPL-VDN:** Similar to O-MAPL but uses standard VDN for value aggregation without learnable mixing networks. 
    * **SL-MARL (Supervised Learning MARL):** A two-phase approach: learns a reward function via supervised learning, then trains a policy using an offline MARL algorithm (OMIGA). 

## Environments

* **SMACv1 (StarCraft Multi-Agent Challenge):** Decentralized micromanagement scenarios in StarCraft II.  Tasks include 2c\_vs\_64zg, 5m\_vs\_6m, 6h\_vs\_8z, and corridor. 
* **SMACv2:** An improved SMAC benchmark with increased diversity through randomized start positions, unit types, and adjusted sight/attack ranges.  Tasks are grouped by factions (Protoss, Terran, Zerg) and unit counts. 
* **MaMujoco (Multi-agent MuJoCo):** Continuous cooperative control tasks for multi-limbed robots.  Tasks include Hopper-v2, Ant-v2, and HalfCheetah-v2. 

## Experimental Results Highlights

O-MAPL demonstrates consistent and significant performance improvements:

* **SMACv1 & SMACv2:** Achieves the highest win rates across most tasks, particularly excelling in complex scenarios like `2c_vs_64zg` (74.4% rule-based, 79.5% LLM-based win rate), `corridor` (93.2% rule-based, 94.5% LLM-based), `protoss_10_vs_11`, and `terran_20_vs_20`.  The use of LLM-generated data generally leads to higher win rates compared to rule-based data. 
* **MaMujoco:** Outperforms all baselines in continuous control tasks, achieving higher returns in `Hopper-v2`, `Ant-v2`, and `HalfCheetah-v2`. For instance, in `Hopper-v2`, O-MAPL achieved a return of 1114.4 ± 154.1, compared to the next best (SL-MARL) at 890.0 ± 88.7. 
* **Faster Convergence:** O-MAPL often converges faster and achieves high performance at earlier training stages. 

For comprehensive results, including learning curves and detailed tables, please refer to Section 6 and the Appendix of our paper, and the `saved_results_final` directory and `analyze.ipynb` notebook in this repository.

## Citation

If you find this work useful in your research, please consider citing our paper:

```bibtex
@inproceedings{bui2025omapl,
  title={O-MAPL: Offline Multi-agent Preference Learning},
  author={Bui, The Viet and Mai, Tien and Nguyen, Thanh Hong},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2025},
}
```

## License

This project is licensed under the MIT License. See the `LICENSE` file for more details.